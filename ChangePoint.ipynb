{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKkZl66rlH6B0yURX+2UUS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3vyfao1N9FR","executionInfo":{"status":"ok","timestamp":1713344999341,"user_tz":-480,"elapsed":491,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}},"outputId":"11a7b3ba-1845-4327-959b-0b1a88db305c"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'PyTorch-LSTM-for-RUL-Prediction' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/jiaxiang-cheng/PyTorch-LSTM-for-RUL-Prediction"]},{"cell_type":"code","source":["import os\n","\n","# 打印当前工作目录\n","print(\"当前工作目录：\", os.getcwd())\n","\n","\n","# 如果需要，更改工作目录\n","os.chdir('/content/PyTorch-LSTM-for-RUL-Prediction')\n","\n","\n","# 列出当前工作目录下的所有文件和文件夹\n","print(os.listdir('.'))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AiQPW-POCwI","executionInfo":{"status":"ok","timestamp":1713345000630,"user_tz":-480,"elapsed":366,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}},"outputId":"65525746-d981-4980-de39-5a1fec209a26"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["当前工作目录： /content/PyTorch-LSTM-for-RUL-Prediction\n","['requirements.txt', '.git', '.idea', '.DS_Store', 'PyTorch-LSTM-for-RUL-Prediction', 'CMAPSSData', '_trials', 'loading_data.pyc', 'LICENSE', '__pycache__', 'main.py', 'README.md', 'visualize.py', 'model.py', 'loading_data.py', '.gitignore']\n"]}]},{"cell_type":"code","source":["\"\"\"Loading data sets\"\"\"\n","import pandas as pd\n","\n","def add_rul_1(df):\n","    \"\"\"\n","    :param df: 原始数据框架\n","    :return: 标记目标的数据框架\n","    \"\"\"\n","    # 对每个单元获取总循环次数\n","    grouped_by_unit = df.groupby(by=\"unit_nr\")\n","    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n","    print(max_cycle)\n","    # 将最大循环次数合并回原始框架\n","    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n","    # 计算每行的剩余使用寿命（线性片段）\n","    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n","    result_frame[\"RUL\"] = remaining_useful_life\n","    # 删除不再需要的最大循环列\n","    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n","    return result_frame\n","\n","def load_FD001(cut):\n","    \"\"\"\n","    :param cut: 目标RUL的上限\n","    :return: 按样本分组的数据\n","    \"\"\"\n","    # 定义文件路径\n","    dir_path = './CMAPSSData/'\n","    # 定义列名便于索引\n","    index_names = ['unit_nr', 'time_cycles']\n","    setting_names = ['setting_1', 'setting_2', 'setting_3']\n","    sensor_names = ['s_{}'.format(i) for i in range(1, 22)]\n","    col_names = index_names + setting_names + sensor_names\n","    # 读取数据\n","    train = pd.read_csv((dir_path + 'train_FD001.txt'), sep='\\s+', header=None, names=col_names)\n","    test = pd.read_csv((dir_path + 'test_FD001.txt'), sep='\\s+', header=None, names=col_names)\n","    y_test = pd.read_csv((dir_path + 'RUL_FD001.txt'), sep='\\s+', header=None, names=['RUL'])\n","    # 基于EDA去除非信息特征\n","    drop_sensors = ['s_1', 's_5', 's_6', 's_10', 's_14', 's_16', 's_18', 's_19']\n","    drop_labels = setting_names + drop_sensors\n","    train.drop(labels=drop_labels, axis=1, inplace=True)\n","\n","    # 选择训练数据中的索引列（单元号和循环次数）\n","    title = train.iloc[:, 0:2]\n","    # 选择训练数据中的数据列\n","    data = train.iloc[:, 2:]\n","\n","    # 对数据进行最小-最大规范化\n","    data_norm = (data - data.min()) / (data.max() - data.min())  # 最小-最大规范化\n","    train_norm = pd.concat([title, data_norm], axis=1)\n","\n","    # 添加剩余使用寿命(RUL)列\n","    train_norm = add_rul_1(train_norm)\n","\n","    # 按单元号分组\n","    group = train_norm.groupby(by=\"unit_nr\")\n","\n","    # 对测试数据重复上述删除特征和规范化的过程\n","    test.drop(labels=drop_labels, axis=1, inplace=True)\n","    title = test.iloc[:, 0:2]\n","    data = test.iloc[:, 2:]\n","    data_norm = (data - data.min()) / (data.max() - data.min())\n","    test_norm = pd.concat([title, data_norm], axis=1)\n","\n","    # 按单元号对测试数据分组\n","    group_test = test_norm.groupby(by=\"unit_nr\")\n","\n","    # 返回处理后的训练数据组、测试数据组和测试数据的真实RUL值\n","    return group, group_test, y_test\n"],"metadata":{"id":"oCbt9YYhOFlL","executionInfo":{"status":"ok","timestamp":1713330960061,"user_tz":-480,"elapsed":372,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","class LSTM1(nn.Module):\n","    \"\"\"LSTM architecture\"\"\"\n","    # 初始化函数定义模型的基本结构\n","    def __init__(self, input_size, hidden_size, num_layers, seq_length=1):\n","        super(LSTM1, self).__init__()\n","        self.input_size = input_size  # 输入层大小\n","        self.hidden_size = hidden_size  # 隐藏层大小\n","        self.num_layers = num_layers  # LSTM层数\n","        self.seq_length = seq_length  # 序列长度\n","\n","        # 定义LSTM层\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,\n","                            dropout=0.1)\n","        # 定义全连接层\n","        self.fc_1 = nn.Linear(hidden_size, 16)  # 第一全连接层，16个节点\n","        self.fc_2 = nn.Linear(16, 8)  # 第二全连接层，8个节点\n","        self.fc = nn.Linear(8, 1)  # 输出层\n","\n","        # 定义Dropout和ReLU激活函数\n","        self.dropout = nn.Dropout(0.1)\n","        self.relu = nn.ReLU()\n","\n","    # 前向传播函数\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: 输入特征\n","        :return: 预测结果\n","        \"\"\"\n","        # 初始化隐藏状态和内部状态\n","        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n","        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n","\n","        # LSTM层输出\n","        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n","\n","        # 选择最后一层的隐藏状态\n","        hn_o = torch.Tensor(hn.detach().numpy()[-1, :, :])\n","        hn_o = hn_o.view(-1, self.hidden_size)\n","        hn_1 = torch.Tensor(hn.detach().numpy()[1, :, :])\n","        hn_1 = hn_1.view(-1, self.hidden_size)\n","\n","        # 应用全连接层和激活函数\n","        out = self.relu(self.fc_1(self.relu(hn_o + hn_1)))\n","        out = self.relu(self.fc_2(out))\n","        out = self.dropout(out)\n","        out = self.fc(out)\n","        return out\n"],"metadata":{"id":"OJBcOr0XOI3Z","executionInfo":{"status":"ok","timestamp":1713330961472,"user_tz":-480,"elapsed":3,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# 函数 testing_function\n","# 定义了一个用于测试模型的函数，接收参数为测试集数量和测试数据分组。\n","# 在一个循环中，对每个测试样本进行预测，并计算预测值和真实值之间的均方根误差（RMSE）。\n","# 使用torch.Tensor将测试数据转换为张量，并通过模型进行预测。\n","# 计算每个预测值与实际值的误差，累加后计算RMSE。\n","# 返回所有测试样本的预测结果和计算出的RMSE。\n","# 函数 train\n","# 接收初始化后的模型、训练集样本数量和训练数据分组作为参数。\n","# 定义了一个循环，用于执行多个训练周期（epochs）。\n","# 在每个epoch内部，使用train()方法设置模型到训练模式。\n","# 遍历所有训练样本，将数据转换为张量，执行前向传播，计算损失，进行反向传播，并更新模型权重。\n","# 每个epoch结束时，调用测试函数评估模型性能，打印损失和RMSE，如果模型在测试集上的性能没有改进，则提前终止训练。\n","# 返回训练过程中得到的最佳结果和对应的RMSE。\n","\n","\n","\n","\n","\"\"\"RUL Prediction with LSTM\"\"\"\n","# from loading_data import *\n","# from model import *\n","# from visualize import *\n","import numpy as np\n","\n","N_HIDDEN = 96  # NUMBER OF HIDDEN STATES\n","N_LAYER = 4  # NUMBER OF LSTM LAYERS\n","N_EPOCH = 150  # NUM OF EPOCHS\n","MAX = 135  # UPPER BOUND OF RUL\n","LR = 0.01  # LEARNING RATE\n","\n","\n","def testing_function(num, group_for_test):\n","    rmse_test, result_test = 0, list()  # 初始化RMSE为0，result_test为空列表，用于存储测试结果。\n","\n","    for ite in range(1, num + 1):  # 遍历测试数据集中的每一个样本。\n","        X_test = group_for_test.get_group(ite).iloc[:, 2:]  # 从测试数据中获取特征数据。\n","        X_test_tensors = Variable(torch.Tensor(X_test.to_numpy()))  # 将测试数据转换为PyTorch张量。\n","        X_test_tensors = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))  # 调整张量形状以适应模型输入。\n","\n","        test_predict = model.forward(X_test_tensors)  # 使用模型进行前向传播得到预测结果。\n","        data_predict = max(test_predict[-1].detach().numpy(), 0)  # 获取最后一次的预测值，并确保其不小于0。\n","        result_test.append(data_predict)  # 将预测结果添加到结果列表中。\n","        rmse_test = np.add(np.power((data_predict - y_test.to_numpy()[ite - 1]), 2), rmse_test)  # 计算RMSE。\n","\n","    rmse_test = (np.sqrt(rmse_test / num)).item()  # 计算最终的RMSE值。\n","    return result_test, rmse_test  # 返回所有测试样本的预测结果和RMSE。\n","\n","def train(model_for_train, ntrain, group_for_train):\n","    rmse_temp = 100  # 初始化临时RMSE为100，作为性能比较的基准。\n","\n","    for epoch in range(1, N_EPOCH + 1):  # 进行多个训练周期的迭代。\n","\n","        model_for_train.train()  # 将模型设置为训练模式。\n","        epoch_loss = 0  # 初始化该周期的损失总和为0。\n","\n","        for i in range(1, ntrain + 1):  # 遍历训练集中的每一个样本。\n","            X, y = group_for_train.get_group(i).iloc[:, 2:-1], group_for_train.get_group(i).iloc[:, -1:]  # 提取特征和目标变量。\n","            X_train_tensors = Variable(torch.Tensor(X.to_numpy()))  # 将特征数据转换为张量。\n","            y_train_tensors = Variable(torch.Tensor(y.to_numpy()))  # 将目标数据转换为张量。\n","            X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))  # 调整张量形状。\n","\n","            outputs = model_for_train(X_train_tensors)  # 进行前向传播得到输出。\n","            optimizer.zero_grad()  # 清除之前的梯度信息。\n","            loss = criterion(outputs, y_train_tensors)  # 计算损失。\n","            epoch_loss += loss.item()  # 累加损失。\n","            loss.backward()  # 进行反向传播计算梯度。\n","            optimizer.step()  # 根据梯度更新模型参数。\n","\n","        if epoch % 1 == 0:  # 每个周期结束后评估模型性能。\n","            model_for_train.eval()  # 设置模型为评估模式。\n","            result, rmse = testing_function(num_test, group_test)  # 调用测试函数得到性能指标。\n","\n","            if rmse_temp < rmse and rmse_temp < 25:  # 如果新的RMSE没有改善且低于25，则停止训练。\n","                result, rmse = result_temp, rmse_temp\n","                break\n","\n","            rmse_temp, result_temp = rmse, result  # 更新最佳RMSE和结果。\n","            print(\"Epoch: %d, loss: %1.5f, rmse: %1.5f\" % (epoch, epoch_loss / ntrain, rmse))  #打印每个周期的损失和RMSE。\n","\n","    return result, rmse  # 返回训练过程中的最佳结果和对应的RMSE。\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"zkFUf6eqONOT","executionInfo":{"status":"error","timestamp":1713337633283,"user_tz":-480,"elapsed":352,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}},"outputId":"a648b497-d6fb-40a3-aed6-fffbad299b77"},"execution_count":22,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid character '，' (U+FF0C) (<ipython-input-22-18d83b22cde7>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-18d83b22cde7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    定义了一个用于测试模型的函数，接收参数为测试集数量和测试数据分组。\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '，' (U+FF0C)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def visualize(result, y_test, num_test, rmse):\n","    \"\"\"\n","    可视化预测结果函数\n","    :param result: 预测结果列表\n","    :param y_test: 测试集的真实RUL值\n","    :param num_test: 样本数量\n","    :param rmse: 预测结果的均方根误差\n","    \"\"\"\n","    result = y_test.join(pd.DataFrame(result))  # 将预测结果与真实RUL值合并为一个DataFrame\n","    result = result.sort_values('RUL', ascending=False)  # 按照RUL值降序排序\n","\n","    # 提取真实和预测的RUL值\n","    true_rul = result.iloc[:, 0].to_numpy()  # 真实的剩余使用寿命\n","    pred_rul = result.iloc[:, 1].to_numpy()  # 预测的剩余使用寿命\n","\n","    plt.figure(figsize=(10, 6))  # 设置图形大小\n","    plt.axvline(x=num_test, c='r', linestyle='--')  # 绘制红色虚线标示训练集大小\n","\n","    plt.plot(true_rul, label='Actual RUL')  # 绘制真实RUL曲线\n","    plt.plot(pred_rul, label='Predicted RUL (RMSE = {})'.format(round(rmse, 3)))  # 绘制预测RUL曲线，并显示RMSE\n","    plt.title('Remaining Useful Life Prediction')  # 图表标题\n","    plt.legend()  # 显示图例\n","    plt.xlabel(\"Samples\")  # X轴标签\n","    plt.ylabel(\"Remaining Useful Life\")  # Y轴标签\n","    plt.savefig('./_trials/{} RUL Prediction with LSTM.png'.format(round(rmse, 3)))  # 保存图形到文件\n","    plt.show()  # 显示图形\n"],"metadata":{"id":"4HvwJfPgOPIK","executionInfo":{"status":"ok","timestamp":1713337617016,"user_tz":-480,"elapsed":3,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # 从数据集中获取基本信息\n","    group, group_test, y_test = load_FD001(MAX)  # 加载数据集并进行预处理，返回分组数据和测试数据的真实RUL值\n","    num_train, num_test = len(group.size()), len(group_test.size())  # 计算训练集和测试集中的样本数\n","    input_size = group.get_group(1).shape[1] - 3  # 计算模型输入特征的数量\n","\n","    # 初始化LSTM模型\n","    model = LSTM1(input_size, N_HIDDEN, N_LAYER)  # 实例化LSTM模型，指定输入大小、隐藏层大小和层数\n","    criterion = torch.nn.MSELoss()  # 定义损失函数为均方误差\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # 定义优化器为Adam，设置学习率\n","\n","    # 训练并评估模型\n","    result, rmse = train(model, num_train, group)  # 调用训练函数，训练模型并获取测试结果和RMSE\n","    visualize(result, y_test, num_test, rmse)  # 调用可视化函数，显示预测结果和真实值的对比图\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"id":"KpYW6w3NOiBs","executionInfo":{"status":"error","timestamp":1713330969338,"user_tz":-480,"elapsed":4012,"user":{"displayName":"xu jiashuai","userId":"00106432762378173594"}},"outputId":"c4836700-a9a9-4109-85c2-00fa9913c472"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["unit_nr\n","1      192\n","2      287\n","3      179\n","4      189\n","5      269\n","      ... \n","96     336\n","97     202\n","98     156\n","99     185\n","100    200\n","Name: time_cycles, Length: 100, dtype: int64\n","Epoch: 1, loss: 10740.83138, rmse: 52.05190\n","Epoch: 2, loss: 4566.35310, rmse: 53.26039\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-b0b2ca949d76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-97403a2a99ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_for_train, ntrain, group_for_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mX_train_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_for_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculate the gradient, manually setting to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-aa58060cc78a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# internal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lstm with input, hidden, and internal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mhn_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    879\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"lD-F3lhhOkJr"},"execution_count":null,"outputs":[]}]}